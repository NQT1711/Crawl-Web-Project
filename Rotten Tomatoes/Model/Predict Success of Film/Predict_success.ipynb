{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Flatten, Dense, Softmax, Dropout\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Softmax, Dropout\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nhập thông tin phim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tên phim\n",
    "film_title = input('Nhập tên phim')\n",
    "\n",
    "# Thể loại phim\n",
    "genre = input('Thể loại phim:')\n",
    "\n",
    "# Rating phim\n",
    "rating = input('Rating phim:')\n",
    "\n",
    "\n",
    "# Tóm tắt phim\n",
    "synopsis = input('Tóm tắt phim:')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tính điểm số sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl tweet về phim trên twitter dựa vào tên phim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mở trình duyệt Google Chrome ở trang thái Tab ẩn danh\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "# options.add_argument('--incognito')\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver', options=options)\n",
    "driver.implicitly_wait(0)\n",
    "actions = ActionChains(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://twitter.com/login\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = WebDriverWait(driver, 600).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'input[autocomplete=\"username\"]')))\n",
    "html_of_interest = driver.execute_script('return arguments[0].innerHTML',element)\n",
    "soup = BeautifulSoup(html_of_interest, 'lxml')\n",
    "\n",
    "sleep(1)\n",
    "driver.find_element(By.CSS_SELECTOR, 'input[autocomplete=\"username\"]').send_keys('cap246k291@gmail.com')\n",
    "actions.send_keys(Keys.ENTER)\n",
    "actions.perform()\n",
    "\n",
    "sleep(5)\n",
    "try:\n",
    "    driver.find_element(By.CSS_SELECTOR, 'input[autocapitalize=\"none\"]').send_keys('@cap246k291')\n",
    "    actions.send_keys(Keys.ENTER)\n",
    "    actions.perform()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "driver.find_element(By.CSS_SELECTOR, 'input[autocomplete=\"current-password\"]').send_keys('cap2@2023')\n",
    "actions.send_keys(Keys.ENTER)\n",
    "actions.perform()\n",
    "\n",
    "sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.CSS_SELECTOR, 'input[aria-label=\"Search query\"]').send_keys('(#%s) lang:en'%title_film)\n",
    "actions.send_keys(Keys.ENTER)\n",
    "actions.perform()\n",
    "\n",
    "list_tweet = []\n",
    "\n",
    "while len(list(set(list_tweet))) < 100:\n",
    "    element = WebDriverWait(driver, 600).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[aria-label=\"Timeline: Search timeline\"]')))\n",
    "    html_of_interest = driver.execute_script('return arguments[0].innerHTML',element)\n",
    "    soup = BeautifulSoup(html_of_interest, 'lxml')\n",
    "\n",
    "    rv = soup.select('div[data-testid=\"tweetText\"]')\n",
    "    for i in rv:\n",
    "        list_tweet.append(i.text)\n",
    "    \n",
    "    a = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid=\"tweetText\"]')\n",
    "    a[-1].location_once_scrolled_into_view\n",
    "\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = pd.DataFrame(list(set(list_tweet)), columns=['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb = pd.read_csv('..\\Sentiment Reviews Film\\Reviews IMDB Dataset.csv')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "text_clsf = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])\n",
    "\n",
    "# Tính điểm số sentiment\n",
    "sentiment_score = (text_clsf.predict(data_tweet['Tweet'].values).tolist().count('positive')/len(data_tweet['Tweet']))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
